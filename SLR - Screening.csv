ID,Authors,Title,Abstract,Year,Source title,Link,Haowei,Category: [LLM/GPT/BERT in RE]  [Review/Unrelated/Unclear],Jati,Category: [LLM/GPT/BERT in RE]  [Review/Unrelated/Unclear],Inclusion:[YES|NO],Haowei,Type of paper,Aspects of requirements,Type of RE activities,Modeling type,Model Type,Training Data,Fine-tuning,Prompt Engineering,Metrics,Role of LLM / GPT*,Challenge,Jati,Type of paper,Type of requirements,Type of RE activities,Modeling type,Application domain,Review Status,Final reviewer,Inclusion:[YES|NO],Type of paper,Type of requirements,Type of RE activities,Modeling type,Application domain
1,Spoletini P.; Ferrari A.,The Return of Formal Requirements Engineering in the Era of Large Language Models,"[ContextandMotivation] Large Language Models (LLMs) have made remarkable advancements in emulating human linguistic capabilities, showing potential
  in executing various traditional software engineering tasks, including code generation. [Question/Problem] Despite their generally good performance, utilizing
  LLM-generated code raises legitimate concerns regarding its correctness and the assurances it can provide. [Principal Idea/Results] To address these concerns,
  we propose turning to formal requirements engineering—a practice currently predominantly used in developing complex systems where adherence to standards 
 and accountability are required. [Contribution] In this vision paper, we discuss the integration of automatic formal requirements engineering techniques as a 
 complement to LLM code generation. Additionally, we explore how LLMs can facilitate the broader acceptance of formal requirements, thus making the vision
  proposed in this paper realizable. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",2024,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),https://link.springer.com/content/pdf/10.1007/978-3-031-57327-9.pdf (PAGE:344),Haowei,LLM4RE Review,,RE4LLM,NO,,,,,,,,,,,,,,,,,,,,,,,,,,
2,Tikayat Ray A.; Cole B.F.; Pinon Fischer O.J.; Bhat A.P.; White R.T.; Mavris D.N.,Agile Methodology for the Standardization of Engineering Requirements Using Large Language Models,"The increased complexity of modern systems is calling for an integrated and comprehensive approach to system design and development and, in particular, a shift 
 toward Model-Based Systems Engineering (MBSE) approaches for system design. The requirements that serve as the foundation for these intricate systems are still 
primarily expressed in Natural Language (NL), which can contain ambiguities and inconsistencies and suffer from a lack of structure that hinders their direct translation 
into models. The colossal developments in the field of Natural Language Processing (NLP), in general, and Large Language Models (LLMs), in particular, can serve as
 an enabler for the conversion of NL requirements into machine-readable requirements. Doing so is expected to facilitate their standardization and use in a model-based
 environment. This paper discusses a two-fold strategy for converting NL requirements into machine-readable requirements using language models. The first approach
 involves creating a requirements table by extracting information from free-form NL requirements. The second approach consists of an agile methodology that facilitates
 the identification of boilerplate templates for different types of requirements based on observed linguistic patterns. For this study, three different LLMs are utilized. 
Two of these models are fine-tuned versions of Bidirectional Encoder Representations from Transformers (BERTs), specifically, aeroBERT-NER and aeroBERT-Classifier,
 which are trained on annotated aerospace corpora. Another LLM, called flair/chunk-english, is utilized to identify sentence chunks present in NL requirements. All three
 language models are utilized together to achieve the standardization of requirements. The effectiveness of the methodologies is demonstrated through the semi-automated
 creation of boilerplates for requirements from Parts 23 and 25 of Title 14 Code of Federal Regulations (CFRs). © 2023 by the authors.",2023,Systems,https://www.mdpi.com/2079-8954/11/7/352,Haowei,BERTinRE,,BERT in RE,NO,,,,,,,,,,,,,,,,,,,,,,,,,,
3,Zhang J.; Chen Y.; Liu C.; Niu N.; Wang Y.,Empirical Evaluation of ChatGPT on Requirements Information Retrieval Under Zero-Shot Setting,"Recently, various illustrative examples have shown the impressive ability of generative large language models (LLMs) to perform NLP related tasks. ChatGPT
  undoubtedly is the most representative model. We empirically evaluate ChatGPT's performance on requirements information retrieval (IR) tasks to derive insights 
 into designing or developing more effective requirements retrieval methods or tools based on generative LLMs. We design an evaluation framework considering
  four different combinations of two popular IR tasks and two common artifact types. Under zero-shot setting, evaluation results reveal ChatGPT's promising ability
  to retrieve requirements relevant information (high recall) and limited ability to retrieve more specific requirements information (low precision). Our evaluation of 
 ChatGPT on requirements IR under zero-shot setting provides preliminary evidence for designing or developing more effective requirements IR methods or tools 
 based on generative LLMs. © 2023 IEEE.",2023,"Proceedings of 2023 2nd International Conference on Intelligent Computing and Next Generation Networks, ICNGN 2023",https://arxiv.org/abs/2304.12562,Haowei,GPTinRE,,GPTinRE,YES,Checked,Conference,"Functional Suitability（Functional Completeness， Functional Correctness）
Reliability（Fault Tolerance， Availability）",Elicitation，Analysis,No specific,GPT-3.5-turbo,"From: Public dataset
Size: Total 2199 pieces",No,YES - Zero-shot prompting ,precision、recall,"ChatGPT has the potential to retrieve the 
relevant information under the setting of zero samples","Limited Precision: Poor performance 
when distinguishing specific non -functional 
requirements information",,,,,,,,,,,,,,
4,Arulmohan S.; Meurs M.-J.; Mosser S.,Extracting Domain Models from Textual Requirements in the Era of Large Language Models,"Requirements Engineering is a critical part of the software lifecycle, describing what a given piece of software will do (functional) and how it will do it (non-functional).
Requirements documents are often textual, and it is up to software engineers to extract the relevant domain models from the text, which is an error-prone and time-consuming task. 
Considering the recent attention gained by Large Language Models (LLMs), we explored how they could support this task. This paper investigates how such models can be used to
 extract domain models from agile product backlogs and compare them to (i) a state-of-practice tool as well as (ii) a dedicated Natural Language Processing (NLP) approach, on top
 of a reference dataset of 22 products and 1, 679 user stories. Based on these results, this
 paper is a first step towards using LLMs and/or tailored NLP to support automated requirements engineering thanks to model extraction using artificial intelligence. © 2023 IEEE.",2023,"Proceedings - 2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion, MODELS-C 2023",https://macsphere.mcmaster.ca/handle/11375/28836,Haowei,GPTinRE,,LLMinRE,YES,Checked,Conference,"Functional Suitability（Functional Completeness， Functional Correctness）
Performance Efficiency（Time Behaviour）",Analysis ,No specific（NLP4RE）,GPT-3.5-turbo,"From：Public dataset published by Dalpiaz et al..
Size： Total 1459 corpus",No,YES - Zero-shot prompting ,F-score,ChatGPT's potential in requirements information retrieval,"1. unconscious bias may introduced during annotation process
2. it is very hard to identify which elements influence the model 
positively or negatively.",,,,,,,,,,,,,,
5,Ronanki K.; Berger C.; Horkoff J.,Investigating ChatGPT's Potential to Assist in Requirements Elicitation Processes,"Natural Language Processing (NLP) for Requirements Engineering (RE) (NLP4RE) seeks to apply NLP tools, techniques, and resources to the RE process to increase
  the quality of the requirements. There is little research involving the utilization of Generative AI-based NLP tools and techniques for requirements elicitation. In 
 recent times, Large Language Models (LLM) like ChatGPT have gained significant recognition due to their notably improved performance in NLP tasks. To explore
  the potential of ChatGPT to assist in requirements elicitation processes, we formulated six questions to elicit requirements using ChatGPT. Using the same six questions,
 we conducted interview-based surveys with five RE experts from academia and industry and collected 30 responses containing requirements. The quality of these 36 responses
 (human-formulated + ChatGPT-generated) was evaluated over seven different requirements quality attributes by another five RE experts through a second round of interview-based 
surveys. In comparing the quality of requirements generated by ChatGPT with those formulated by human experts, we found that ChatGPT-generated requirements are highly Abstract,
 Atomic, Consistent, Correct, and Understandable. Based on these results, we present the most pressing issues related to LLMs and what future research should focus on to leverage 
the emergent behaviour of LLMs more effectively in natural language-based RE activities. © 2023 IEEE.",2023,"Proceedings - 2023 49th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2023",https://arxiv.org/abs/2307.07381,Haowei,GPTinRE,,GPTinRE,YES,checked,Conference,"Functional Suitability（Functional Completeness， Functional Correctness）
Interaction Capability(Learnability, Operability)",Elicitation,No specific,GPT-3.5,"From：Round1-interview-based survey;
Round2-GPT 
Size:30/6. Total 36",No,YES - Used few-shot prompting with context-specific examples,"Correctness/Consistency/Understandability/

Unambiguity/Feasibility",Reduce the workload of artificial experts in preliminary requirements elicitation,"Unambiguity and Feasibility of the requirements received lower scores

Ambiguous presentation of requirements may lead to uncertainty in the 
decision-making process during the design stage

Feasibility is important to consider as a requirement is only of value 
if it can be transformed into a design and an implementation with reasonable effort and cost",,,,,,,,,,,,,,
6,Jain C.; Anish P.R.; Singh A.; Ghaisas S.,A Transformer-based Approach for Abstractive Summarization of Requirements from Obligations in Software Engineering Contracts,"Software Engineering (SE) contracts are a valuable source of software requirements. Seed requirements derived from SE contracts can provide a starting point to the
 Requirements Engineering (RE) phase. To extract such a seed however, a correct interpretation of contracts text is crucial. A major challenge with contracts text interpretation
 is that the text is lengthy, convoluted, and it incorporates a complex Legalese. If a summary of the high-level requirements from obligations present in SE contracts is available 
to the requirement analysts in a language that is comprehensible to them, they can use this seed requirements knowledge to ask the right questions to the stakeholders. 
In this paper, we propose an approach for summarizing the requirements present in obligations in a language comprehensible to requirement analysts. We use the principles 
of Prompt Engineering to prompt GPT-3 to generate summaries for training Natural Language Generation (NLG) models for generating SE-specific summaries. Experiments 
using NLG models such as BART, GPT-2, T5, and Pegasus indicate that Pegasus generates the most accurate summaries with the highest ROUGE score as compared to other models. © 2023 IEEE.",2023,Proceedings of the IEEE International Conference on Requirements Engineering,A Transformer-based Approach for Abstractive Summarization of Requirements from Obligations in Software Engineering Contracts | IEEE Conference Publication | IEEE Xplore,Haowei,LLMinRE,,LLMinRE,YES,Checked,Conference,"Functional Suitability（functional completeness ，functional appropriateness）
Interaction Capability（Learnability， Operability）",Elicitation,No specific,GPT-3 (generate initial training summary),"From: 251 expired SE contracts from 13 diverse application domains

Size: 
initial 251 SE contracts -> 57200 contractual statements
Finally 5587 statements",YES,YES-Zero-shot,ROUGE,Help requirement analysts by providing concise and comprehensible summaries of complex contract text,"Accuracy of Summaries: GPT-3 generated summaries required manual refinement to ensure accuracy and completeness.
Security: Using large models like GPT-3 poses data privacy and storage concerns, hence not used for final summarization.",,,,,,,,,,,,,,
7,Zhang X.; Liu L.; Wang Y.; Liu X.; Wang H.; Ren A.; Arora C.,PersonaGen: A Tool for Generating Personas from User Feedback,"Personas are crucial in software development processes, particularly in agile settings. However, no effective tools are available for generating personas from user 
 feedback in agile software development processes. To fill this gap, we propose a novel tool that uses the GPT-4 model and knowledge graph to generate persona 
templates from well-processed user feedback, facilitating requirement analysis in agile software development processes. We developed a tool called PersonaGen. 
We evaluated PersonaGen using qualitative feedback from a small-scale user study involving student software projects. The results were mixed, highlighting challenges
 in persona-based educational practice and addressing non-functional requirements. © 2023 IEEE.",2023,Proceedings of the IEEE International Conference on Requirements Engineering,https://ieeexplore.ieee.org/document/10260883,Haowei,GPTinRE,,GPTinRE,YES？,Checked,Conference,effeciency,Elicitation,No specific,GPT-4,"From student software projects
Size: 3 projects",No,YES- few shot,qualitative feedback from 13 students,"Personagen helps generate high-quality persona templates, and promote requirements acquisition process","
It is diffcult to analyze non-functional requirements",,,,,,,,,,,,,,
8,Fantechi A.; Gnesi S.; Passaro L.; Semini L.,Inconsistency Detection in Natural Language Requirements using ChatGPT: a Preliminary Evaluation,"With the rapid advancement of tools based on Artificial Intelligence, it is interesting to assess their usefulness in requirements engineering. In early experiments, 
 we have seen that ChatGPT can detect inconsistency defects in natural language (NL) requirements, that traditional NLP tools cannot identify or can identify with 
difficulties even after domain-focused training. This study is devoted to specifically measuring the performance of ChatGPT in finding inconsistency in requirements. 
Positive results in this respect could lead to the use of ChatGPT to complement existing requirements analysis tools to automatically detect this important quality criterion. 
For this purpose, we consider GPT-3.5, the Generative Pretrained Transformer language model developed by OpenAI. We evaluate its ability to detect inconsistency 
by comparing its predictions with those obtained from expert judgments by students with a proven knowledge of RE issues on a few example requirements documents. © 2023 IEEE.",2023,Proceedings of the IEEE International Conference on Requirements Engineering,https://ieeexplore.ieee.org/document/10260964,Haowei,GPTinRE,,GPTinRE,YES,Checked,Conference,"Interaction Capability（Operability）
Reliability（Fault Tolerance，  Availability）",analysis & validation,No specific,GPT-3.5,"From：authors， company, agency
Size: 167 rqs",NO,YES-Zero-shot,precision、recall,"ChatGPT can complement manual analysis, particularly excelling in detecting some conflicts that experts might overlook. 
However, manual analysis  performs better in terms of precision and recall.","Lack of explainability and transparency
Document length limit
Manual verification of the results is still required to ensure accuracy",,,,,,,,,,,,,,
9,Gorer B.; Aydemir F.B.,Generating Requirements Elicitation Interview Scripts with Large Language Models,"Requirements elicitation interviews are the most popular requirements elicitation technique and an integral part of requirements engineering education. Good and
  bad interview scripts provide students with examples of applying the theory. Constructing an interview script requires technical knowledge, practical experience, and creativity. 
As a result, only a few educational interview scripts are available to the community. This paper explores automatically generating interview scripts with large language models through
 prompt engineering. Our contribution is two-fold: First, we present a graph representation of interactive interview scripts. Second, we apply prompt engineering techniques to generate 
business domain descriptions, linear scripts, and conversation pieces focused on certain types of mistakes. Our findings indicate that large language models face challenges in handling 
interview conversation graphs. However, we can enhance the quality of the generated interview scripts by decomposing the task into smaller components and refining the prompts to 
provide more precise instructions. © 2023 IEEE.",2023,"Proceedings - 31st IEEE International Requirements Engineering Conference Workshops, REW 2023",https://ieeexplore.ieee.org/document/10260795,Haowei,LLMinRE,,GPTinRE,YES,Checked,Workshop,"Functional Suitability（functional completeness ，functional appropriateness）
Interaction Capability（Learnability， Operability， Appropriateness Recognizability）",Elicitation,No specific,"GPT-3.5
Google Bard",From：interview script by Debnath and Spoletini,NO,"YES 
few shot and chain-of-thought prompting",Semantic Similarity Metrics：BERTScore，Cosine Similarity of Sentence Embeddings,"LLM can complement traditional requirements elicitation methods, particularly excelling in generating interview scripts. They are effective in capturing typical analyst mistakes and generating corresponding script segments.

"," LLMs face difficulties in processing complex interview conversation graphs
Complex tasks require careful prompt design",,,,,,,,,,,,,,
10,Clements D.; Giannis E.; Crowe F.; Balapitiya M.; Marshall J.; Papadopoulos P.; Kanij T.,An Innovative Approach to Develop Persona from Application Reviews,"Software end users are diverse by nature and their different facets influence the way they use software. An understanding of the users and their needs are achieved
  by engaging with the users during requirement engineering. However, sometimes recruiting users during requirement engineering phase can be very challenging. An accessible 
way to understand a user's perspective and traits is through user application reviews. This research paper proposes an innovative approach to develop user personas from a data 
set of e-commerce application user reviews by using GPT-3 and PATHY. This enables the development teams to see different demographic data, as well as overall frustrations and 
expectations that users of their platform possess, so developers know how to enhance their software solutions. This is also helpful to developers of new e-commerce applications. 
Copyright © 2023 by SCITEPRESS - Science and Technology Publications, Lda. Under CC license (CC BY-NC-ND 4.0)",2023,"International Conference on Evaluation of Novel Approaches to Software Engineering, ENASE - Proceedings",https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjFzYn7mqOGAxXPZvUHHfuhCF4QFnoECBQQAQ&url=https%3A%2F%2Fresearch.monash.edu%2Ffiles%2F501919780%2F472857069_oa.pdf&usg=AOvVaw3ADhcrUqjjd1jUk__Ot7i8&opi=89978449,Haowei,GPTinRE,,GPTinRE,YES,Checked,Conference,"Functional Suitability（functional completeness ，functional appropriateness）
Performance Efficiency（Time Behaviour， Resource Utilization）",elicitation,No specific,GPT-3,"From：Application reviews were extracted from 25 different e-
commerce applications from open source app store and play store
Size：4999  （4931 were analysed）",NO,"YES
Zero shot",information power+qualitative,"It is beneficial for developers of new e-commerce platforms to understand the diverse user facets and their needs.
","GPT-3 may face challenges when processing reviews with spelling errors or incomplete
 information, which can affect the accuracy of feature extraction.",,,,,,,,,,,,,,
11,Cruciani F.; Moore S.; Nugent C.,Comparing general purpose pre-trained Word and Sentence embeddings for Requirements Classification,"The recent evolution of NLP has enriched the set of DL-based approaches to include a number of general-purpose Large Language Models (LLMs). Whereas new
  models have been proven useful for generic text handling, their applicability to domain-specific NLP tasks still remains doubtful, particularly because of the limited amount of 
dataset available in certain domains, such as Requirements Engineering. In this study, different pre-trained embeddings were tested in three requirements classification tasks, 
in search of a tradeoff between accuracy and computational complexity. The best F1-score results were obtained with BERT (90.36% and 84.23%), with DistilBERT identified as 
optimal tradeoff (90.28% and 82.61%). © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",2023,CEUR Workshop Proceedings,https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiC78HznKOGAxV-hq8BHSbgCvcQFnoECBAQAQ&url=https%3A%2F%2Fceur-ws.org%2FVol-3378%2FNLP4RE-paper4.pdf&usg=AOvVaw0WWoc-YGp3PIE2k0NCPzrs&opi=89978449,Haowei,NLP with RE,,BERTinRE,NO,,,,,,,,,,,,,,,,,,,,,,,,,,
12,Luitel D.; Hassani S.; Sabetzadeh M.,Improving requirements completeness: automated assistance through large language models,"Natural language (NL) is arguably the most prevalent medium for expressing systems and software requirements. Detecting incompleteness in NL requirements is a major challenge. One approach to identify incompleteness is to compare requirements with external sources. Given the rise of large language models (LLMs), an interesting question arises: Are LLMs useful external sources of knowledge for detecting potential incompleteness in NL requirements? This article explores 
 this question by utilizing BERT. Specifically, we employ BERT’s masked language model to generate contextualized predictions for filling masked slots in requirements. To simulate incompleteness, we withhold content from the requirements and assess BERT’s ability to predict terminology that is present in the withheld content but absent in the disclosed content. BERT can produce multiple predictions per mask. Our first contribution is determining the optimal number of predictions per mask, striking a balance between effectively identifying omissions in requirements and mitigating noise present in the predictions. Our second contribution involves designing a machine learning-based filter to post-process BERT’s predictions and further reduce noise. We conduct an empirical evaluation using 40 requirements specifications from the PURE dataset. Our findings indicate that: (1) BERT’s predictions effectively highlight terminology that is missing from requirements, (2) BERT outperforms simpler baselines in identifying relevant yet missing terminology, and (3) our filter reduces noise in the predictions, enhancing BERT’s effectiveness for completeness checking of requirements. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.",2024,Requirements Engineering,https://link.springer.com/article/10.1007/s00766-024-00416-3,Haowei,BERTinRE,,BERTinRE,NO,,,,,,,,,,,,,,,,,,,,,,,,,,
13,Borg M.,Requirements Engineering and Large Language Models: Insights From a Panel,"As a general-purpose technology, large language models promise to enhance various software engineering tasks. But how will they impact requirements engineering? This column offers a summary of an expert panel discussion from the 2023 International Requirements Engineering Conference in Hanover, Germany. © 1984-2012 IEEE.",2024,IEEE Software,https://ieeexplore.ieee.org/document/10452860,Haowei,LLMinRE Survey,,LLMinRE Survey (Panel discussion report),NO,,,,,,,,,,,,,,,,,,,,,,,,,,
14,Wang W.; Xing B.,Research on the Integration and Application of Design Thinking and Large Language Models in the Innovation Design of Fintech Products,"This study explores the integration and application of Design Thinking and Large Language Models (LLMs) in fintech product innovation design from the perspective
  of requirements management. Design thinking is human-centered, focusing on discovering and solving problems, integrating innovative solutions, and enhancing product value. By understanding and generating natural language, LLMs provide ""on-demand"" intelligent services through information retrieval, content creation, and human-like dialogue, meeting individualized needs. Through literature review and analysis, this paper reveals that the combination of the two can promote the full intelligence and continuous improvement of the innovation design process, enhancing the R&D efficiency and user experience of fintech products. This research aims to provide innovation design methods for the fintech field and promote its continuous innovation and development. © 2024 The Authors.",2024,Frontiers in Artificial Intelligence and Applications,https://scholar.archive.org/work/kdc3scayarhznddyhvczpgoazm,Haowei,LLMinRE(?) Survey,,LLMinRE Survey,NO,,,,,,,,,,,,,,,,,,,,,,,,,,
15,Mehraj A.; Zhang Z.; Systä K.,A Tertiary Study on AI for Requirements Engineering,"Context and Motivation: Rapid advancements in Artificial Intelligence (AI) have significantly influenced requirements engineering (RE) practices. Problem: While 
 many recent secondary studies have explored AI’s role in RE, a thorough understanding of the use of AI for RE (AI4RE) and its inherent challenges remains in its early stages.Principal Ideas: To fill this knowledge gap, we conducted a tertiary review on understanding how AI assists RE practices. Contribution: We analyzed 28 secondary studies from 2017 to September 2023 about using AI in RE tasks such as elicitation, classification, analysis, specification, management, and tracing. Our study reveals a trend of combining natural language process techniques with machine learning models like Latent Dirichlet Allocation (LDA) and Naive Bayes, and a surge in using large language models (LLMs) for RE. The study also identified challenges of AI4RE related to ambiguity, language, data, algorithm, and evaluation. The study gives topics for future research, particularly for researchers who want to start new research in this field. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",2024,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),https://ieeexplore.ieee.org/document/10154972,Haowei,AI in RE-> Unrelated,,LLM in SE,NO,,,,,,,,,,,,,,,,,,,,,,,,,,
16,Yang Y.; Zhang Q.; Li C.; Marta D.S.; Batool N.; Folkesson J.,Human-Centric Autonomous Systems With LLMs for User Command Reasoning,"The evolution of autonomous driving has made remark-able advancements in recent years, evolving into a tangible reality. However, a human-centric large-scale
  adoption hinges on meeting a variety of multifaceted requirements. To ensure that the autonomous system meets the user's intent, it is essential to accurately discern and interpret user commands, especially in complex or emergency situations. To this end, we propose to leverage the reasoning capabilities of Large Language Models (LLMs) to infer system requirements from in-cabin users' commands. Through a series of experiments that include different LLM models and prompt designs, we explore the few-shot mul-tivariate binary classification accuracy of system requirements from natural language textual commands. We confirm the general ability of LLMs to understand and reason about prompts but underline that their effectiveness is conditioned on the quality of both the LLM model and the design of appropriate sequential prompts. Code and models are public with the link ht tps: / / github. com/KTH-RPL/Dri veCmd_LLM. © 2024 IEEE.",2024,"Proceedings - 2024 IEEE Winter Conference on Applications of Computer Vision Workshops, WACVW 2024",https://ieeexplore.ieee.org/document/10495632,Haowei,LLMinRE,,Unclear (Is human-centered design here part of RE?),YES->NO,,Workshop,Reliability,elicitation,No specific,"GPT-3.5
GPT-4
CodeLlama-34b-Instruct
Llama-2-70b-Chat","From: UCU dataset
Size: 1099",No,YES few shot,"Command Level Accuracy
Question Level Accuracy","LLM can improve autonomous driving systems' understanding and 
response to user commands, particularly in enhancing the reliability of requirements.","The model may face challenges when processing commands containing 
ambiguous or complex information, which can affect reasoning accuracy.

The need for multi-step reasoning means that errors at each step can accumulate, 
affecting the overall accuracy and reliability of the final result.",,,,,,,,,,,,,,
17,Belzner L.; Gabor T.; Wirsing M.,"Large Language Model Assisted Software Engineering: Prospects, Challenges, and a Case Study","Large language models such as OpenAI’s GPT and Google’s Bard offer new opportunities for supporting software engineering processes. Large language model
  assisted software engineering promises to support developers in a conversational way with expert knowledge over the whole software lifecycle. Current applications range from requirements extraction, ambiguity resolution, code and test case generation, code review and translation to verification and repair of software vulnerabilities. In this paper we present our position on the potential benefits and challenges associated with the adoption of language models in software engineering. In particular, we focus on the possible applications of large language models for requirements engineering, system design, code and test generation, code quality reviews, and software process management. We also give a short review of the state-of-the-art of large language model support for software construction and illustrate our position by a case study on the object-oriented development of a simple “search and rescue” scenario. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.",2024,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),https://link.springer.com/chapter/10.1007/978-3-031-46002-9_23 ,Haowei,LLMinRE(Survey),,LLM in SE,NO,,,,,,,,,,,,,,,,,,,,,,,,,,
18,Tikayat Ray A.; Cole B.F.; Pinon Fischer O.J.; White R.T.; Mavris D.N.,aeroBERT-Classifier: Classification of Aerospace Requirements Using BERT,"The system complexity that characterizes current systems warrants an integrated and comprehensive approach to system design and development. This need has 
 brought about a paradigm shift towards Model-Based Systems Engineering (MBSE) approaches to system design and a departure from traditional document-centric methods. While MBSE shows great promise, the ambiguities and inconsistencies present in Natural Language (NL) requirements hinder their conversion to models directly. The field of Natural Language Processing (NLP) has demonstrated great potential in facilitating the conversion of NL requirements into a semi-machine-readable format that enables their standardization and use in a model-based environment. A first step towards standardizing requirements consists of classifying them according to the type (design, functional, performance, etc.) they represent. To that end, a language model capable of classifying requirements needs to be fine-tuned on labeled aerospace requirements. This paper presents an open-source, annotated aerospace requirements corpus (the first of its kind) developed for the purpose of this effort that includes three types of requirements, namely design, functional, and performance requirements. This paper further describes the use of the aforementioned corpus to fine-tune BERT to obtain the aeroBERT-Classifier: a new language model for classifying aerospace requirements into design, functional, or performance requirements. Finally, this paper provides a comparison between aeroBERT-Classifier and other text classification models such as GPT-2, Bidirectional Long Short-Term Memory (Bi-LSTM), and bart-large-mnli. In particular, it shows the superior performance of aeroBERT-Classifier on classifying aerospace requirements over existing models, and this is despite the fact that the model was fine-tuned using a small labeled dataset. © 2023 by the authors.",2023,Aerospace,https://www.mdpi.com/2226-4310/10/3/279,Haowei,BERTinRE,,BERTinRE,NO,,,,,,,,,,,,,,,,,,,,,,,,,,
19,Minina P.; Sadovykh A.; Ivanov V.,Detecting Security Requirements in GitHub Issues -Novel Dataset and SmallBERT-based model,"Cloud application security initiates with the analysis of security requirements in DevOps. This involves gathering, managing, and tracking requirements within
  integrated issue-tracking systems found in repositories like GitHub. DevOps offers advantages in cloud app development, such as accelerated deployment, improved collaboration, and enhanced reliability. In DevOps, while many security verification tools are automated, security requirements analysis often relies on manual procedures. User feedback plays a pivotal role in shaping cloud application requirements, and the industry actively seeks automation solutions to expedite development. Prior research has demonstrated the limited performance of conventional NLP models trained on established datasets, such as PROMISE, when employed in the context of GitHub Issues. Recent studies have explored the integration of deep learning, particularly leveraging modern large language models and transfer learning architectures, to address requirements engineering challenges. However, a significant issue persists - the transferability of these models. While these models excel when applied to datasets similar to those they were trained on, their performance often drastically falls when dealing with external domains.In our paper, we introduce an automated method for classifying requirements within issue trackers. This method utilizes a novel dataset comprising 12,000 security and non-security issues collected from open GitHub repositories. We employed a SmallBERT-based model for training and conducted a series of experiments. Our research reaffirms the challenge related to the transferability of NLP models. Simultaneously, our model yields highly promising results when applied to GitHub Issues, even in challenging scenarios involving issues from projects that were not part of the training dataset and structured requirements texts from the PROMISE dataset. In summary, our approach significantly contributes to enhancing DevOps practices within cloud applications by automating security requirements analysis. © 2023 IEEE.",2023,"Proceedings of the International Conference on Cloud Computing Technology and Science, CloudCom",https://ieeexplore.ieee.org/document/10475803,Haowei,BERTinRE,,BERTinRE,NO,,,,,,,,,,,,,,,,,,,,,,,,,,
20,Blasek N.; Eichenmüller K.; Ernst B.; Götz N.; Nast B.; Sandkuhl K.,Large language models in requirements engineering for digital twins,"Can large language models (LLMs) be used for digital twin engineering (DTE)? Engineering digital twins (DTs) is a complex process consisting of several phases
  and involving different disciplines. We argue that an investigation of LLM use in DTE has to define what kinds of DTs are in focus and what DTE phases shall be supported. In our work, we concentrate on the early phases of DTE, with a particular focus on requirements engineering (RE), and we focus on supervisory and operational DTs. This paper investigates the quality of LLM output for defining requirements for DTs. The main contributions of our work are results from an experiment comparing requirements to a DT of an air conditioning facility of a domain expert and ChatGPT and conclusions for prompt engineering resulting from this experiment. © 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",2023,"Companion Proceedings of the 16th IFIP WG 8.1 Working Conference on the Practice of Enterprise Modeling and the 13th
Enterprise Design and Engineering Working Conference, November 28 – December 1, 2023, Vienna, Austria",https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwi41Lvy56eGAxVNh68BHQFTD9sQFnoECBEQAQ&url=https%3A%2F%2Fceur-ws.org%2FVol-3645%2Fdte1.pdf&usg=AOvVaw2DvzmxE61OmNFIN35xL2oD&opi=89978449,Haowei,GPTinRE,,GPTinRE,YES,,Conference,"Reliability（Faultlessness， Consistency）
Performance Efficiency（Time Behaviour， Resource Utilization）",elicitation,No specific,ChatGPT-4,"From：The researchers designed the questionnaires
Size: 17 questionaires",No,YES few shot,qualitative analysis,"ChatGPT can serve as a supplementary tool for domain experts, 
saving time and effort in the early stages of requirements engineering.",ChatGPT often lacks depth and specificity in certain technical details.,,,,,,,,,,,,,,
21,Fan A.; Gokkaya B.; Harman M.; Lyubarskiy M.; Sengupta S.; Yoo S.; Zhang J.M.,Large Language Models for Software Engineering: Survey and Open Problems,"This paper provides a survey of the emerging area of Large Language Models (LLMs) for Software Engineering (SE). It also sets out open research challenges for 
 the application of LLMs to technical problems faced by software engineers. LLMs' emergent properties bring novelty and creativity with applications right across the spectrum of Software Engineering activities including coding, design, requirements, repair, refactoring, performance improvement, documentation and analytics. However, these very same emergent properties also pose significant technical challenges; we need techniques that can reliably weed out incorrect solutions, such as hallucinations. Our survey reveals the pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in the development and deployment of reliable, efficient and effective LLM-based SE. © 2023 IEEE.",2023,"Proceedings - 2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering, ICSE-FoSE 2023",https://arxiv.org/abs/2310.03533,Haowei,LLMinRE,,LLM in SE,NO,,,,,,,,,,,,,,,,,,,,,,,,,,
22,Wei J.; Courbis A.-L.; Lambolais T.; Xu B.; Bernard P.L.; Dray G.,Zero-shot Bilingual App Reviews Mining with Large Language Models,"App reviews from app stores are crucial for improving software requirements. A large number of valuable reviews are continually being posted, describing software 
 problems and expected features. Effectively utilizing user reviews necessitates the extraction of relevant information, as well as their subsequent summarization. Due to the substantial volume of user reviews, manual analysis is arduous. Various approaches based on natural language processing (NLP) have been proposed for automatic user review mining. However, the majority of them requires a manually crafted dataset to train their models, which limits their usage in real-world scenarios. In this work, we propose Mini-BAR, a tool that integrates large language models (LLMs) to perform zero-shot mining of user reviews in both English and French. Specifically, Mini-BAR is designed to (i) classify the user reviews, (ii) cluster similar reviews together, (iii) generate an abstractive summary for each cluster and (iv) rank the user review clusters. To evaluate the performance of Mini-BAR, we created a dataset containing 6,000 English and 6,000 French annotated user reviews and conducted extensive experiments. Preliminary results demonstrate the effectiveness and efficiency of Mini-BAR in requirement engineering by analyzing bilingual app reviews. © 2023 IEEE.",2023,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",https://arxiv.org/abs/2311.03058,Haowei,Unrelated,,LLM in RE (If we consider review analysis part of requirements gathering),NO,,,,,,,,,,,,,,,,,,,,,,,,,,
23,Virvou M.; Tsihrintzis G.A.,Pre-made Empowering Artificial Intelligence and ChatGPT: The Growing Importance of Human AI-Experts,"This paper investigates the augmented responsibility of human Artificial Intelligence experts in the era of empowered pre-made Artificial Intelligence (AI). The responsible and ethical use of pre-made AI is of paramount importance in this evolving technology. AI systems have the potential to impact numerous aspects of society, ranging from healthcare and finance to education and IoT. The decisions made by AI algorithms can have significant consequences for individuals, communities, and even entire industries. Using a comparison to the way widely available medicines require a prescription from medical doctors, human AI experts assume the role of evaluating, recommending, and overseeing the implementation of AI systems, even when pre-built AI solutions may seem user-friendly on the surface. The paper has explored the expanded responsibilities of human AI experts within two contemporary scenarios involving pre-made AI, encompassing LLMs and ChatGPT. These
  AI technologies are applied in two principal manners: initially, as standalone AI products readily accessible to a wide audience, and secondly, as elements undergoing exploration for integration into other AI-driven software and Intelligent Information Systems (IIS), with the goal of enhancing natural language processing (NLP) features within user interfaces. In all cases, the expertise of human AI professionals is indispensable, and their role is augmented. These professionals bear an increased responsibility for ensuring the responsible and ethical deployment of AI technologies, with a focus on human-centered design, bias mitigation, validation and accuracy estimation of the results, transparency promotion, and the necessary balance between automation and human oversight. This paper performs a review on pre-made AI and ChatGPT together with custom-based AI and shows that recent advance require an augmented role of human AI experts © 2023 IEEE.",2023,"14th International Conference on Information, Intelligence, Systems and Applications, IISA 2023",https://ieeexplore.ieee.org/document/10345880,Haowei,LLM survey,,Unrelated,NO,,,,,,,,,,,,,,,,,,,,,,,,,,
24,Nguyen-Duc A.; Arora C.; Abrahamsson P.,Preface of RESET 2023: 2nd International Workshop on Requirement Engineering for Software Startups and Emerging Technologies,"The Second International Workshop on Requirement Engineering for Software startups and Emerging Technologies (RESET) is a part of the 31st IEEE International 
 Requirements Engineering Conference 2023, held on 4 September 2023. The workshop brought together requirements engineering researchers and practitioners to discuss the need for adapting conventional requirement engineering artifacts (i.e., requirement definition, metrics), processes and practices in developing and operating emerging technologies, including Software Startups, Artificial Intelligence (AI), Blockchain, and Quantum Computing. Participants gained insights into the RE practices, tools, techniques, and frameworks that can help them build scalable, robust, and innovative software-intensive systems. The workshop included a keynote presentation and four paper presentations. © 2023 IEEE.",2023,"Proceedings - 31st IEEE International Requirements Engineering Conference Workshops, REW 2023",https://ieeexplore.ieee.org/document/10260766,Haowei,Survey,,Unrelated,NO,,,,,,,,,,,,,,,,,,,,,,,,,,
25,Jin Z.; Niu N.; Yu Y.,An RE'23 Workshop on Environment-Driven Requirements Engineering (EnviRE'23),"We organize a one-day workshop on Environment-Driven Requirements Engineering (EnviRE'23) in conjunction with the 31st IEEE International Requirements 
 Engineering Conference. With the rising influence of AI, IoT, and cyber-physical systems, we realize that the environment, in which the software operates, becomes more open and evolves rapidly with stakeholders' changing needs. EnviRE'23 features one keynote and seven accepted papers. In addition, we organize an interactive session with workshop participants to explore the role of large language models (LLMs), specifically ChatGPT, in requirements elicitation and modeling. Overall, the workshop is aimed at bringing the interested researchers and practitioners together, exchanging ideas and visions, and exploring a set of open problems to pursue in the years to come. Since the first edition of EnviRE in 2021, this is the first time that our workshop is held in person. We are excited to continue our workshop after the pandemic. © 2023 IEEE.",2023,"Proceedings - 31st IEEE International Requirements Engineering Conference Workshops, REW 2023",https://ieeexplore.ieee.org/document/10260948,Haowei,Survey,,Unrelated,NO,,,,,,,,,,,,,,,,,,,,,,,,,,
26,Chen B.; Chen K.; Hassani S.; Yang Y.; Amyot D.; Lessard L.; Mussbacher G.; Sabetzadeh M.; Varro D.,On the Use of GPT-4 for Creating Goal Models: An Exploratory Study,"The emergence of large language models and conversational front-ends such as ChatGPT is revolutionizing many software engineering activities. The extent to 
 which such technologies can help with requirements engineering activities, especially the ones surrounding modeling, however, remains to be seen. This paper reports on early experimental results on the potential use of GPT-4 in the latter context, with a focus on the development of goal-oriented models. We first explore GPT-4's current knowledge and mastering of a specific modeling language, namely the Goal-oriented Requirement Language (GRL). We then use four combinations of prompts - with and without a proposed textual syntax, and with and without contextual domain knowledge - to guide the creation of GRL models for two case studies. The first case study focuses on a well-documented topic in the goal modeling community (Kids Help Phone), whereas the second one explores a context for which, to our knowledge, no public goal models currently exist (Social Housing). We explore the interactive construction of a goal model through specific follow-up prompts aimed to fix model issues and expand on the model content. Our results suggest that GPT-4 preserves considerable knowledge on goal modeling, and although many elements generated by GPT-4 are generic, reflecting what is already in the prompt, or even incorrect, there is value in getting exposed to the generated concepts, many of which being non-obvious to stakeholders outside the domain. Furthermore, aggregating results from multiple runs yields a far better outcome than from any individual run. © 2023 IEEE.",2023,"Proceedings - 31st IEEE International Requirements Engineering Conference Workshops, REW 2023",https://ieeexplore.ieee.org/document/10260905,Haowei,GPTinRE,,GPTinRE,YES,,Workshop,"Functional Suitability（functional completeness ，functional appropriateness）
Interaction Capability（Learnability， Operability）",Analysis & Validation,No specific,ChatGPT-4,From：Public generic domain data,No,YES - Used few-shot,qualitative analysis,"The paper has a significant impact on the field of requirements engineering, particularly in goal modeling. 
It can improve the efficiency and accuracy of the requirements engineering process","The output of GPT-4 exhibits random variation, resulting in inconsistent results across different runs.
The generated models may be overly generic, lacking sufficient domain specificity, and thus may not effectively identify conflicts among stakeholders.",,,,,,,,,,,,,,
27,Bertram V.; Kausch H.; Kusmenko E.; Nqiri H.; Rumpe B.; Venhoff C.,Leveraging Natural Language Processing for a Consistency Checking Toolchain of Automotive Requirements,"In the automotive industry, specifications often consist of a large number of textual requirements. These requirements are linguistically ambiguous and written in
  informal language. Utilizing Structured English for requirements eliminates ambiguity, improves data quality, and supports further automated processing while maintaining readability. The recent development of large language models enables a fully automated translation approach using few-shot learning. To deal with the limited context size of large language models, an improved algorithm, OptKATE, is presented to find an ideal set of requirements for few-shot learning. Structured English can be used as a basis for further formalization. This capability is key in creating an interface between natural language processing and verification, in our case, consistency analysis using the Z3 SMT solver. We implemented a grammar for translating Structured English into TCTL using the MontiCore workbench. Furthermore, since SMT-based methods currently rely on manual precondition satisfaction and do not tackle conflicting preconditions automatically, we propose a scenario generation algorithm that generates potential scenarios using the specification and checks the requirements against them. Through this approach, we can better identify and resolve conflicting preconditions, ultimately improving the consistency of requirements. Our toolchain is evaluated using an automotive requirements dataset provided by former Daimler AG. © 2023 IEEE.",2023,Proceedings of the IEEE International Conference on Requirements Engineering,https://ieeexplore.ieee.org/document/10260788,Haowei,Not related （NLP4RE）,,Unrelated,NO,,,,,,,,,,,,,,,,,,,,,,,,,,
28,Cosler M.; Hahn C.; Mendoza D.; Schmitt F.; Trippel C.,nl2spec: Interactively Translating Unstructured Natural Language to Temporal Logics with Large Language Models,"A rigorous formalization of desired system requirements is indispensable when performing any verification task. This often limits the application of verification
  techniques, as writing formal specifications is an error-prone and time-consuming manual task. To facilitate this, we present nl2spec, a framework for applying Large Language Models (LLMs) to derive formal specifications (in temporal logics) from unstructured natural language. In particular, we introduce a new methodology to detect and resolve the inherent ambiguity of system requirements in natural language: we utilize LLMs to map subformulas of the formalization back to the corresponding natural language fragments of the input. Users iteratively add, delete, and edit these sub-translations to amend erroneous formalizations, which is easier than manually redrafting the entire formalization. The framework is agnostic to specific application domains and can be extended to similar specification languages and new neural models. We perform a user study to obtain a challenging dataset, which we use to run experiments on the quality of translations. We provide an open-source implementation, including a web-based frontend. © 2023, The Author(s).",2023,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),https://arxiv.org/abs/2303.04864,Haowei,LLMinRE,,LLMinRE,YES,,Arxiv,"Functional Suitability（functional completeness ，Functional Correctness）
Reliability（Fault Tolerance， Faultlessness）",Specification,No specific,Codex (a variant of GPT-3) and Bloom,"From：Public generic domain data
Codex：159GB code
Bloom：Training set 1.61TB text",No,YES - Used few-shot,accuracy," The paper has a significant impact on formal verification. LLMs can translate natural language into formal specifications, 
so that it can improve the efficiency and accuracy of the requirements specification process.","Ambiguity Resolution: The inherent ambiguity of natural language needs to be resolved through interactive methods.
Initial Translation Errors: Initial translations may contain errors that require human intervention to correct.",,,,,,,,,,,,,,
29,Mustroph H.; Barrientos M.; Winter K.; Rinderle-Ma S.,Verifying Resource Compliance Requirements from Natural Language Text over Event Logs,"Process compliance aims to ensure that processes adhere to requirements imposed by natural language texts such as regulatory documents. Existing approaches
  assume that requirements are available in a formalized manner using, e.g., linear temporal logic, leaving the question open of how to automatically extract and formalize them for verification. Especially with the constantly growing amount of regulatory documents and their frequent updates, it can be preferable to provide an approach that enables the verification of processes with requirements in natural language text instead of formalized requirements. To this end, this paper presents an approach that copes with the verification of resource compliance requirements, e.g., which resource shall perform which activity, in natural language over event logs. The approach relies on a comprehensive literature analysis to identify resource compliance patterns. It then contrasts these patterns with resource patterns reflecting the process perspective, while considering the natural language perspective. We combine the state-of-the-art GPT-4 technology for pre-processing the natural language text with a customized compliance verification component to identify and verify resource compliance requirements. Thereby, the approach distinguishes different resource patterns including multiple organizational perspectives. The approach is evaluated based on a set of well-established process descriptions and synthesized event logs generated by a process execution engine as well as the BPIC 2020 dataset. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",2023,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"https://link.springer.com/chapter/10.1007/978-3-031-41620-0_15
（Page 249·）",Haowei,GPTinRE,,GPTinRE,YES,,Conference,Reliability（Faultlessness， Consistency）,verification,No,GPT-4,From： PET dataset,No,YES - Used few-shot,Precision， Recall,use the output of GPT w.r.t. compliance verification over event logs,"GPT-4 is a black box model, which makes ensuring reproducibility challenging due to its dependence on finely-tuned prompts",,,,,,,,,,,,,,
30,Bashir S.; Abbas M.; Saadatmand M.; Enoiu E.P.; Bohlin M.; Lindberg P.,"Requirement or Not, That is the Question: A Case from the Railway Industry","[Context and Motivation] Requirements in tender documents are often mixed with other supporting information. Identifying requirements in large tender 
 documents could aid the bidding process and help estimate the risk associated with the project. [Question/problem] Manual identification of requirements in large documents is a resource-intensive activity that is prone to human error and limits scalability. This study compares various state-of-the-art approaches for requirements identification in an industrial context. For generalizability, we also present an evaluation on a real-world public dataset. [Principal ideas/results] We formulate the requirement identification problem as a binary text classification problem. Various state-of-the-art classifiers based on traditional machine learning, deep learning, and few-shot learning are evaluated for requirements identification based on accuracy, precision, recall, and F1 score. Results from the evaluation show that the transformer-based BERT classifier performs the best, with an average F1 score of 0.82 and 0.87 on industrial and public datasets, respectively. Our results also confirm that few-shot classifiers can achieve comparable results with an average F1 score of 0.76 on significantly lower samples, i.e., only 20% of the data. [Contribution] There is little empirical evidence on the use of large language models and few-shots classifiers for requirements identification. This paper fills this gap by presenting an industrial empirical evaluation of the state-of-the-art approaches for requirements identification in large tender documents. We also provide a running tool and a replication package for further experimentation to support future research in this area. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",2023,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"https://link.springer.com/chapter/10.1007/978-3-031-29786-1_8
（Page 105～）",Haowei,BERTinRE,,BERTinRE,NO,,,,,,,,,,,,,,,,,,,,,,,,,,
31,Grasler I.; Preus D.; Brandt L.; Mohr M.,Efficient Extraction of Technical Requirements Applying Data Augmentation,"Requirements for complex technical systems are documented in natural language sources. Manually extracting requirements from these documents-e.g., to 
 transfer them to a requirements management tool-is time-consuming and error-prone. Today, machine learning approaches are used to classify natural language requirements and thus enable extraction of these requirements. However, in practice there is often not enough labeled domain-specific data available to train such models. For this reason, this work investigates the performance in artificially generating requirements through data augmentation. First, success criteria for a method for extracting and augmenting requirements are elicited in cooperation with industry experts. Second, the performance in the augmentation of requirements data is investigated. The results show that GPT-J is suitable for generating artificial requirements: weighted average F1-score: 62.74 %. Third, a method is developed to extract requirements from specifications, augment requirements data, and then classify the requirements. As a final step, the method is evaluated with requirements data from three industry case examples of the engineering service provider EDAG Engineering GmbH: Assembly latch hood, adjustable stopper hood and trunk curtain roller blind. Evaluation shows that especially the transferability of models is improved when they are trained with augmented data. The developed method facilitates eliciting complete requirements sets. Performance of artificial intelligence models in requirements extraction is improved applying augmented data and therefore the method leads to efficient product development. © 2022 IEEE.",2022,"ISSE 2022 - 2022 8th IEEE International Symposium on Systems Engineering, Conference Proceedings",https://ieeexplore.ieee.org/abstract/document/10005452,Haowei,GPTinRE,,GPTinRE,YES,,Conference,"Performance Efficiency（Time Behaviour， Resource Utilization）
Reliability（Fault Tolerance， Accuracy）","Elicitation, Specification",No,GPT-J,"From：Open source software development project
Size：793",No,YES  few-shot,F1-score,The developed method can be used to improve the applicability of requirements extraction in industrial practice.,The results of this paper indicate that using augmented data for training from a different domain than test data leads to decreased performance.,,,,,,,,,,,,,,
32,Ajagbe M.; Zhao L.,Retraining a BERT Model for Transfer Learning in Requirements Engineering: A Preliminary Study,"In recent years, advanced deep learning language models such as BERT, ELMO, ULMFiT and GPT have demonstrated strong performance on many general natural
  language processing (NLP) tasks. BERT, in particular, has also achieved promising results on some domain-specific tasks, including the requirements classification task. However, in spite of its great potential, BERT under-performs on domain specific tasks. In this paper, we present BERT4RE, a BERT-based model retrained on requirements texts, aiming to support a wide range of requirements engineering (RE) tasks, including classifying requirements, detecting language issues, identifying key domain concepts, and establishing requirements traceability links. We demonstrate the transferability of BERT4RE, by fine-tuning it for the task of identifying key domain concepts. Our preliminary study shows that BERT4RE achieved better results than the BERTbase model on the demonstrated RE task. © 2022 IEEE.",2022,Proceedings of the IEEE International Conference on Requirements Engineering,https://ieeexplore.ieee.org/document/9920081,Haowei,BERT4RE,,BERTinRE,NO,,,,,,,,,,,,,,,,,,,,,,,,,,
33,"J. O. Couder, D. Gomez and O. Ochoa",Requirements Verification Through the Analysis of Source Code by Large Language Models,"In the most recent years, Large Language Models (LLMs) have gained popularity and have been accepted and used in different domains due to their ability to 
 understand and generate written language. LLMs allow us to analyze large amounts of data in a few moments, yet they are also extremely simple to use, making them a very powerful assistive tool that can aid in a wide range of tasks; from planning a family trip, to aid during the development process of a huge system. For software developers, LLMs have been mostly used for code generation, explanation, or optimization. Software verification is a crucial part of software development as it is the process of ensuring that a system meets specific requirements. Requirements specifications play a pivotal role in software verification as they define what a system should do. In this paper we propose the use of LLMs for code verification through the analysis of requirements specifications. We prove that LLMs, such as GPT-3.5, can verify a list of requirements through a given code and evaluate why the requirements have or have not been met.",2024,"SoutheastCon 2024, Atlanta, GA, USA, 2024, pp. 75-80",https://ieeexplore.ieee.org/document/10500073,Haowei,LLMinRE,,LLMinRE,YES,,Journal,"Reliability（Faultlessness， Accuracy）
Functional Suitability（Functional Completeness， Functional Correctness）",Verification,No,GPT-3.5,,No,YES  few-shot,,This model simplifies the user story creation process and integrates it into the Agile Requirement Engineering workflow,"The responses generated by large language models can vary significantly, leading to inconsistencies in the user stories.",,,,,,,,,,,,,,
34,"J. U. Oswal, H. T. Kanakia and D. Suktel",Transforming Software Requirements into User Stories with GPT-3.5 -: An AI-Powered Approach,"In today's dynamic software development landscape, Agile methodologies have established themselves as essential for organizations striving to swiftly adapt to 
 evolving customer needs and market demands. A cornerstone of the Agile framework is the concept of User Stories, a concise format for expressing software requirements from an end-user perspective. The manual generation of User Stories from unstructured requirement texts proves to be a labor-intensive endeavor, riddled with challenges related to maintaining consistency and adhering to specific organizational practices. This research underscores the profound importance of Agile Methodology in contemporary software development and underscores the critical role that User Stories play within this framework. To address the inherent inefficiencies associated with manual User Story creation, this paper introduces a novel and innovative AI-powered approach that uses the advanced capabilities of the GPT-3.5 language model. This approach facilitates a seamless and efficient transformation of software requirement text into standardized User Stories by studying various prompting techniques. In this research paper the practical implementation of our approach have been illustrated, we have developed an application harnessing the natural language processing capabilities of GPT-3.5 where in the user can enter or upload the requirement text and it will be transformed into user stories.",2024,2024 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT),https://ieeexplore.ieee.org/document/10467750,Haowei,GPTinRE,,GPTinRE,YES,,Conference,"Functional Suitability（Functional Completeness，Functional Appropriateness)
Maintainability (Modifiability)",Elicitation,No,GPT-3.5,,No,Yes  few shot and zero shot,,"The model reducing the time and effort required to generate user stories, minimizing the risk of human errors and inconsistencies in the user stories.",,,,,,,,,,,,,,,
35,"J. Peer, Y. Mordecai and Y. Reich",NLP4ReF: Requirements Classification and Forecasting: From Model-Based Design to Large Language Models,"We introduce Natural Language Processing for Requirement Forecasting (NLP4ReF), a model-based machine learning and natural language processing solution for enhancing the Requirements Engineering (RE) process. RE continues to face significant challenges and demands innovative approaches for process efficiency. Traditional RE methods relying on natural language struggle with incomplete, hidden, 
 forgotten, and evolving requirements during and after the critical design review, risking project failures and setbacks. NLP4ReF tackles several key challenges: a) distinguishing between functional and non-functional requirements, b) classification of requirements by their respective system classes, and c) generation of unanticipated requirements to enhance project success. NLP4ReF employs a common natural language toolkit (NLTK) package and the recently-trending Chat-GPT. We tested NLP4ReF on PROMISE_exp, a pre-existing dataset with 1000 software requirements, and PROMISE_IoT, an enhanced dataset with 2000 software and IoT requirements. We validated NLP4ReF on a genuine IoT project. NLP4ReF swiftly generated dozens of new requirements, verified by a team of systems engineers, of which over 70% were crucial for project success. We found that GPT is superior in authentic requirement generation, while NLTK excels at requirement classification. NLP4ReF offers significant time saving, effort reduction, and improved future-proofing. Our model-based design approach provides a foundation for enhanced RE practices and future research in this domain.",2024,2024 IEEE Aerospace Conference,https://ieeexplore.ieee.org/document/10521022,Haowei,NLP+GPT inRE,,GPTinRE,YES,,Conference,"Functional Suitability（functional completeness ，Functional Correctness）
Performance Efficiency（Time Behaviour， Resource Utilization）",Elicitation and Classification,No,GPT-3.5 ,"PROMISE_exp dataset：1000 requirements
PROMISE_IoT dataset：2000 requirements",No,Yes  few shot and zero shot,"Precision, recall, F1-score, authenticity, BLEU",The model significantly reduced the time required for generating and classifying requirements.,"The performance of the model in requirements classification was 
affected by noise originating from the extensive training data of the OpenAI GPT",,,,,,,,,,,,,,
36,"J. S. Yeow, M. E. Rana and N. A. Abdul Majid",An Automated Model of Software Requirement Engineering Using GPT-3.5,"While the potential of AI in software development is undeniable, integrating advanced models like GPT-3.5 into its core processes like requirements engineering
  remains largely unexplored. This research investigates the effectiveness of GPT-3.5 in automating key tasks within software requirements engineering. The primary objective is to comprehensively explore the capabilities, limitations, and potential applications of GPT-3.5 in software requirements engineering. Subsequently, the research undergoes thorough analysis and evaluation to gather insights into the strengths and limitations of GPT-3.5 in the requirement-gathering process. The research concludes by identifying the limitations and putting forth recommendations for future research endeavours aimed at integrating GPT-3.5 into software requirement engineering processes. While GPT-3.5 demonstrates proficiency in aspects like creative prototyping and question generation, limitations in areas like domain understanding and context awareness become evident. By outlining these limitations, the authors offer concrete recommendations for future research focusing on the seamless integration of GPT-3.5 and similar models into the broader framework of software requirements engineering.",2024,2024 ASU International Conference in Emerging Technologies for Sustainability and Intelligent Systems (ICETSIS),https://ieeexplore.ieee.org/document/10459458,Haowei,GPTinRE(Survey),,GPTinRE (Survey),NO,,,,,,,,,,,,,,,,,,,,,,,,,,
37,"Madhava Krishna, Bhagesh Gaur, Arsh Verma, Pankaj Jalote",Using LLMs in Software Requirements Specifications: An Empirical Evaluation,"The creation of a Software Requirements Specification (SRS) document is important for any software development project. Given the recent prowess of Large Language Models (LLMs) in answering natural language queries and generating sophisticated textual outputs, our study explores their capability to produce accurate, coherent, and structured drafts of these documents to accelerate the software development lifecycle. We assess the performance of GPT-4 and CodeLlama in drafting an SRS for a university club management system and compare it against human benchmarks using eight distinct criteria. Our
 results suggest that LLMs can match the output quality of an entry-level software engineer to generate an SRS, delivering complete and consistent drafts. We also evaluate the capabilities of LLMs to identify and rectify problems in a given requirements document. Our experiments indicate that GPT-4 is capable of
 identifying issues and giving constructive feedback for rectifying them, while CodeLlama’s results for validation were not as encouraging. We repeated the generation exercise for four distinct use cases to study the time saved by employing LLMs for SRS generation. The experiment demonstrates that LLMs may
 facilitate a significant reduction in development time for entry- level software engineers. Hence, we conclude that the LLMs canbe gainfully used by software engineers to increase productivity by saving time and effort in generating, validating and rectifying software requirements.",2024,,https://arxiv.org/abs/2404.17842,Haowei,LLMinRE,,GPTinRE,YES,,Arxiv,"Functional Suitability（functional completeness ，Functional Correctness）
Performance Efficiency（Time Behaviour， Resource Utilization）","Specification, Validation",N o,GPT-4 and CodeLlama, Internal case studies,No,Yes few shot,5-point Likert scale,The model has good performance by the LLMs for generating SRS documents (RQ1). The generations are consistent and usually complete.,The stochasticity of LLMs outputs complicates the generation process and in determining which combination of the prompt and context works successfully.,,,,,,,,,,,,,,
38,Ali Nouri，Beatriz Cabrero-Daniel，Fredrik T ̈orner，H ̊akan Sivencrona， Christian Berger,Engineering Safety Requirements for Autonomous Driving with Large Language Models,"Changes and updates in the requirement artifacts, which can be frequent in the automotive domain, are a challenge for SafetyOps. Large Language Models (LLMs), 
 with their impressive natural language understanding and generating capabilities, can play a key role in automatically refining and decomposing requirements after each update. In this study, we propose a prototype of a pipeline of prompts and LLMs that receives an item definition and outputs solutions in the form of safety requirements. This pipeline also performs a review of the requirement dataset and identifies redundant or contradictory requirements. We first identified the necessary characteristics for performing HARA and then defined tests to assess an LLM’s capability in meeting these criteria. We used design science with multiple iterations and let experts from different companies evaluate each cycle quantitatively and qualitatively. Finally, the prototype was implemented at a case company and the responsible team evaluated its efficiency.",2024,32nd IEEE International Requirements Engineering 2024 conference,https://arxiv.org/abs/2403.16289,Haowei,LLMinRE,,GPTinRE,YES,,Conference,"Functional Suitability（Functional Completeness）
Safety","Elicitation, Specification, Analysis & Validation",No,GPT-4,Source: Company-specific scenarios and malfunctions,No,Yes few shot,readability and Evaluation criteria with car safety correlation,"It demonstrates the potential of LLMs for the specifying and specifying safety requirements for AD functions in a real world, industrial context,",Have risk of data leakage into the AI’s training data,,,,,,,,,,,,,,
39,"Dongming Jin, Zhi Jin, Xiaohong Chen, Chunhui Wang",MARE: Multi-Agents Collaboration Framework for Requirements Engineering,"Requirements Engineering (RE) is a critical phase in the software development process that generates requirements specifications from stakeholders’ needs. Recently, deep learning techniques have been successful in several RE tasks. However, obtaining high-quality requirements specifications requires collaboration across multiple tasks and roles. In this paper, we propose an innovative framework called MARE, which leverages collaboration among large language models (LLMs) throughout the entire RE process. MARE divides the RE process into four tasks: elicitation, modeling, verification, and specification. Each task is conducted by engaging one or two specific agents and each agent can conduct several actions. MARE has five agents and nine actions. To facilitate collaboration between agents, MARE has designed a workspace for agents to upload their generated intermediate requirements artifacts and obtain the information they need. We conduct experiments on five public cases, one dataset, and four new cases created by this work. We compared MARE with three baselines using three widely used metrics for the generated requirements models. Experimental results show that MARE can generate more correct requirements models and outperform the state-of-the-art approaches by 15.4%. For the generated requirements specifications, we conduct a human evaluation in three aspects and provide insights about the quality.",2024,,https://arxiv.org/abs/2405.03256,Haowei,LLMinRE,,LLMinRE,YES,,Arxiv,"Maintainability（Modularity， Analyzability）
Reliability（Faultlessness， Consistency）","Elicitation, Modeling, Verification, Specification","Use Case Diagrams, Problem Diagrams, Goal Models",GPT-3.5-turbo,"5 public evaluation cases for generating use case diagrams, 1 public dataset for generating goal models, and 4 new evaluation cases for generating problem diagrams",No,Yes few shot,"Precision, recall, F1-score","given a very rough idea of requirements, MARE autonomously and iteratively performs the basic tasks of RE.",Handling complex and domain-specific requirements may pose challenges in maintaining accuracy and completeness.,,,,,,,,,,,,,,
40,"Robert Feldt, Riccardo Coppola",Semantic API Alignment: Linking High-level User Goals to APIs,"Large Language Models (LLMs) are becoming key in automating and assisting various software development tasks, including text-based tasks in requirements engineering but also in coding. Typically, these models are used to automate small portions of existing tasks, but we present a broader vision to span multiple steps from requirements engineering to implementation using existing libraries. This approach, which we call Semantic API Alignment (SEAL), aims to bridge the gap between a user’s high-level goals and the specific functions of one or more APIs.
 In this position paper, we propose a system architecture where a set of LLM-powered “agents” match such high-level objectives with appropriate API calls. This system could facilitate automated programming by finding matching links or, alternatively, explaining mismatches to guide manual intervention or further development.
 As an initial pilot, our paper demonstrates this concept by applying LLMs to Goal-Oriented Requirements Engineering (GORE), via sub-goal analysis, for aligning with REST API specifications, specifically through a case study involving a GitHub statistics API. We discuss the potential of our approach to enhance complex tasks in software development and requirements engineering and outline future directions for research.",2024,"11TH INTERNATIONAL WORKSHOP ON ARTIFICIAL INTELLIGENCE AND REQUIREMENTS ENGINEERING - AIRE, 2024",https://arxiv.org/abs/2405.04236,Haowei,LLMinRE,,LLMinRE,YES,,Conference,"Functional Suitability（functional completeness ，Functional Correctness）
Compatibility（Interoperability）",Analysis & Validation,No,GPT-4,From EMB database,No,Yes few shot,The success rate of alignment between user goals and API endpoints,improve the efficiency and accuracy of the requirements analysis and implementation process in semantic API alignment,"The initially generated API alignments may contain errors, requiring human intervention for correction.",,,,,,,,,,,,,,
41,"Mohammadmehdi Ataei, Hyunmin Cheong, Daniele Grandi, Ye Wang, Nigel Morris, Alexander Tessier",Elicitron: An LLM Agent-Based Simulation Framework for Design Requirements Elicitation,"Requirements elicitation, a critical, yet time-consuming and challenging step in product development, often fails to capture the full spectrum of user needs. This may lead to products that fall short of expectations. This paper introduces a novel framework that leverages Large Language Models (LLMs) to automate and enhance the requirements elicitation process. LLMs are used to generate a vast array of simulated users (LLM agents), enabling the exploration of a much broader range of user needs and unforeseen use cases. These agents engage in product experience scenarios, through explaining their actions, observations, and challenges. Subsequent agent interviews and analysis uncover valuable user needs, including latent ones. We validate our framework with three experiments. First, we explore different methodologies for diverse agent generation, discussing their advantages and shortcomings. We measure the diversity of identified user needs and demonstrate that context-aware agent generation leads to greater diversity. Second, we show how our framework effectively mimics empathic lead user interviews, identifying a greater number of latent needs than conventional human interviews. Third, we showcase that LLMs can be used to analyze interviews, capture needs, and classify them as latent or not. Our work highlights the potential of using LLM agents to accelerate early-stage product development, reduce costs, and increase innovation.",2024,,https://arxiv.org/abs/2404.16045,Haowei,LLMinRE,,LLMinRE,YES,,Arxiv,Safety， Functional Suitability,Elicitation,No,GPT-4-Turbo,,No,Yes (few-shot and chain-of-thought),"Precision, recall, F1-score",LLMs demonstrate effectiveness in analyzing interviews and classifying latent needs.,Quality of insights depends on the LLM’s capabilities,,,,,,,,,,,,,,
42,"Zheying Zhang, Maruf Rayhan, Tomas Herda, Manuel Goisauf, Pekka Abrahamsson",LLM-based agents for automating the enhancement of user story quality: An early report,"In agile software development, maintaining high-quality user stories is crucial, but also challenging. This study explores the use of large language models (LLMs) to automatically improve the user story quality in Austrian Post Group IT agile teams. We developed a reference model for an Autonomous LLM-based Agent System (ALAS), and implemented it at Austrian Post Group IT. The quality of use stories in the study and the effectiveness of these agents were assessed by 11 participants across six agile teams. Our findings demonstrate the potential of LLMs in improving user story quality, contributing to the research on AI’s role in Agile development, and providing a practical example of the transformative impact of AI in an industry setting.",2024,,https://arxiv.org/abs/2403.09442,Haowei,LLMinRE,,LLMinRE,YES,,Arxiv,"Maintainability（Modularity， Analyzability）
Interaction Capability（Appropriateness Recognizability， Operability）",Elicitation,No,GPT-3.5-turbo and GPT-4,"From Austrian Post Group IT agile teams
Size 25 stories",No,Yes few shot,Likert scale（questionnaire）,"LLM improves user story clarity, comprehensibility, and alignment with business objectives",Need fine-tuning parameters to minimize hallucinations and enhance contextual accuracy.,,,,,,,,,,,,,,
43,"Beian Wang, Chong Wang, Peng Liang, Bing Li, Cheng Zeng",How LLMs Aid in UML Modeling: An Exploratory Study with Novice Analysts,"Since the emergence of GPT-3, Large Language Models (LLMs) have caught the eyes of researchers, practitioners, and educators in the field of software engineering. However, there has been relatively little investigation regarding the performance of LLMs in assisting with requirements analysis and UML modeling. This paper explores how LLMs can assist novice analysts in creating three types of typical UML models: use case models, class diagrams, and sequence diagrams. For this purpose, we designed the modeling tasks of these three UML models for 45 undergraduate students who participated in a requirements modeling course, with the help of LLMs. By analyzing their project reports, we found that LLMs can assist undergraduate students as notice analysts in UML modeling tasks, but LLMs also have shortcomings and limitations.",2024,,https://arxiv.org/abs/2404.17739,Haowei,LLMinRE,,LLMinRE,YES,,Arxiv,"Functional Suitability（functional completeness ，Functional Correctness）
Interaction Capability（Learnability， Operability）",Analysis & Validation,UML ,GPT-3.5 and GPT-4,"From：undergraduate students
Size：45",No,Yes few shot,correctness rate,"LLMs can already serve as an auxiliary tool to provide effective suggestions, appropriate guidance, and reference examples for software modeling.",Uncertainty of the LLM outputs,,,,,,,,,,,,,,
44,"Abdelkarim El-Hajjami, Nicolas Fafin, Camille Salinesi","Which AI Technique Is Better to Classify Requirements? An Experiment with SVM, LSTM, and ChatGPT","Recently, Large Language Models like ChatGPT have demonstrated remarkable proficiency in various Natural Language Processing tasks. Their application in Requirements Engineering, especially in requirements classification, has gained increasing interest. This paper reports an extensive empirical evaluation of two ChatGPT models, specifically gpt-3.5-turbo, and gpt-4 in both zero-shot and few-shot settings for requirements classification. The question arises as to how these models compare to traditional classification methods, specifically Support Vector Machine and Long Short-Term Memory. Based on five different datasets, our results show that there is no single best technique for all types of requirement classes. Interestingly, the few-shot setting has been found to be beneficial primarily in scenarios where zero-shot results are significantly low.",2024,Joint Proceedings of REFSQ-2024 Workshop,"[2311.11547] Which AI Technique Is Better to Classify Requirements? An Experiment with SVM, LSTM, and ChatGPT (arxiv.org)",Haowei,GPTinRE,,GPTinRE,YES,,workshop,"Functionality（Functional Correctness， Functional Appropriateness）
Performance Efficiency（Time Behaviour， Resource Utilization）",Specification,No specific,"GPT-3.5-turbo, GPT-4","From：PROMISE, Dronology, ReqView, Leeds Library, WASP
Size：956",No,Yes  few shot and zero shot,𝐹𝛽 score,"By demonstrating the potential of different AI techniques in requirements 
classification tasks, it can improve the efficiency and accuracy of the requirements engineering process.","the sensitivity of ChatGPT’s responses to prompt variations and the non-deterministic nature of its output, 
which could vary slightly even with identical prompts.",,,,,,,,,,,,,,
45,"Krishna Ronanki, Beatriz Cabrero-Daniel, Jennifer Horkoff, Christian Berger",Requirements Engineering using Generative AI: Prompts and Prompting Patterns,"[Context]: Companies are increasingly recognizing the importance of automating Requirements Engineering (RE) tasks due to their resource-intensive nature. The advent of GenAI has made these tasks more amenable to automation, thanks to its ability to understand and interpret context effectively. [Problem]: However, in the context of GenAI, prompt engineering is a critical factor for success. Despite this, we currently lack tools and methods to systematically assess and determine the most effective prompt patterns to employ for a particular RE task. [Method]: Two tasks related to requirements, specifically requirement classification and tracing, were automated using the GPT-3.5 turbo API. The performance evaluation involved assessing various prompts created using 5 prompt patterns and implemented programmatically to perform the selected RE tasks, focusing on metrics such as precision, recall, accuracy, and F-Score. [Results]: This paper evaluates the effectiveness of the 5 prompt patterns' ability to make GPT-3.5 turbo perform the selected RE tasks and offers recommendations on which prompt pattern to use for a specific RE task. Additionally, it also provides an evaluation framework as a reference for researchers and practitioners who want to evaluate different prompt patterns for different RE tasks.",2024,arXiv,https://arxiv.org/abs/2311.03832,Haowei,GPTinRE,,GPTinRE,YES,,Arxiv,"Functional Suitability（functional completeness ，Functional Correctness）
Maintainability（Modularity， Analyzability）","elicitation
Analysis & Validation",No specific,GPT-3.5 turbo,"From：PROMISE（621 requirements）
 / PURE",No,YES - Used few-shot,precision、recall、accuracy，F-score,"By demonstrating the potential of generative AI in requirements classification and traceability tasks, 
it can improve the efficiency and accuracy of the requirements engineering process.","The sensitivity of prompt engineering means that different prompts 
can lead to significantly different results, necessitating careful design and testing.",,,,,,,,,,,,,,
46,"Andreas Vogelsang, Jannik Fischbach",Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline,"Large Language Models (LLMs) are the cornerstone in automating Requirements Engineering (RE) tasks, underpinning recent advancements in the field. Their pre-trained comprehension of natural language is pivotal for effectively tailoring them to specific RE tasks. However, selecting an appropriate LLM from a myriad of existing architectures and fine-tuning it to address the intricacies of a given task poses a significant challenge for researchers and practitioners in the RE domain. Utilizing LLMs effectively for NLP problems in RE necessitates a dual understanding: firstly, of the inner workings of LLMs, and secondly, of a systematic approach to selecting and adapting LLMs for NLP4RE tasks. This chapter aims to furnish readers with essential knowledge about LLMs in its initial segment. Subsequently, it provides a comprehensive guideline tailored for students, researchers, and practitioners on harnessing LLMs to address their specific objectives. By offering insights into the workings of LLMs and furnishing a practical guide, this chapter contributes towards improving future research and applications leveraging LLMs for solving RE challenges.",2024,,https://arxiv.org/abs/2402.13823,Haowei,LLMinRE（Review）,,LLMinRE,NO,,,,,,,,,,,,,,,,,,,,,,,,,,
47,"Alessio Ferrari, Sallam Abualhaija, Chetan Arora",Model Generation from Requirements with LLMs: an Exploratory Study,"Complementing natural language (NL) requirements with graphical models can improve stakeholders' communication and provide directions for system design. However, creating models from requirements involves manual effort. The advent of generative large language models (LLMs), ChatGPT being a notable example, offers promising avenues for automated assistance in model generation. This paper investigates the capability of ChatGPT to generate a specific type of model, i.e., UML sequence diagrams, from NL requirements. We conduct a qualitative study in which we examine the sequence diagrams generated by ChatGPT for 28 requirements documents of various types and from different domains. Observations from the analysis of the generated diagrams have systematically been captured through evaluation logs, and categorized through thematic analysis. Our results indicate that, although the models generally conform to the standard and exhibit a reasonable level of understandability, their completeness and correctness with respect to the specified requirements often present challenges. This issue is particularly pronounced in the presence of requirements smells, such as ambiguity and inconsistency. The insights derived from this study can influence the practical utilization of LLMs in the RE process, and open the door to novel RE-specific prompting strategies targeting effective model generation.",2024,,https://arxiv.org/abs/2404.06371,Haowei,LLMinRE（Review）,,LLMinRE,NO,,,,,,,,,,,,,,,,,,,,,,,,,,
48,"Luitel, D., Hassani, S. & Sabetzadeh, M",Improving requirements completeness: automated assistance through large language models,"Natural language (NL) is arguably the most prevalent medium for expressing systems and software requirements. Detecting incompleteness in NL requirements is a major challenge. One approach to identify incompleteness is to compare requirements with external sources. Given the rise of large language models (LLMs), an interesting question arises: Are LLMs useful external sources of knowledge for detecting potential incompleteness in NL requirements? This article explores this question by utilizing BERT. Specifically, we employ BERT’s masked language model to generate contextualized predictions for filling masked slots in requirements. To simulate incompleteness, we withhold content from the requirements and assess BERT’s ability to predict terminology that is present in the withheld content but absent in the disclosed content. BERT can produce multiple predictions per mask. Our first contribution is determining the optimal number of predictions per mask, striking a balance between effectively identifying omissions in requirements and mitigating noise present in the predictions. Our second contribution involves designing a machine 
 learning-based filter to post-process BERT’s predictions and further reduce noise. We conduct an empirical evaluation using 40 requirements specifications from the PURE dataset. Our findings indicate that: (1) BERT’s predictions effectively highlight terminology that is missing from requirements, (2) BERT outperforms simpler baselines in identifying relevant yet missing terminology, and (3) our filter reduces noise in the predictions, enhancing BERT’s effectiveness for completeness checking of requirements.",2024,,https://arxiv.org/abs/2308.03784,Haowei,BERTinRE,,BERTinRE,NO,,,,,,,,,,,,,,,,,,,,,,,,,,
49,"Sauvola, J., Tarkoma, S., Klemettinen, M. et al.",Future of software development with generative AI ,"Generative AI is regarded as a major disruption to software development. Platforms, repositories, clouds, and the automation of tools and processes have been proven to improve productivity, cost, and quality. Generative AI, with its rapidly expanding capabilities, is a major step forward in this field. As a new key enabling technology, it can be used for many purposes, from creative dimensions to replacing repetitive and manual tasks. The number of opportunities increases with the capabilities of large-language models (LLMs). This has raised concerns about ethics, education, regulation, intellectual property, and even criminal activities. We analyzed the potential of generative AI and LLM technologies for future software development paths. We propose four primary scenarios, model trajectories for transitions between them, and reflect against relevant software development operations. The motivation for this research is clear: the software development industry needs new tools to understand the potential, limitations, and risks of generative AI, as well as guidelines for using it.",2024,Automated Software Engineering,https://link.springer.com/article/10.1007/s10515-024-00426-z,Haowei,GenAIinRE Review,,LLMinSE,NO,,,,,,,,,,,,,,,,,,,,,,,,,,
50,"Chetan Arora, John Grundy, Mohamed Abdelrazek",Advancing Requirements Engineering through Generative AI: Assessing the Role of LLMs,"Requirements Engineering (RE) is a critical phase in software development including the elicitation, analysis, specification, and validation of software requirements. Despite the importance of RE, it remains a challenging process due to the complexities of communication, uncertainty in the early stages and inadequate automation support. In recent years, large-language models (LLMs) have shown significant promise in diverse domains, including natural language processing, code generation, and program understanding. This chapter explores the potential of LLMs in driving RE processes, aiming to improve the efficiency and accuracy of requirements-related tasks. We propose key directions and SWOT analysis for research and development in using LLMs for RE, focusing on the potential for requirements elicitation, analysis, specification, and validation. We further present the results from a preliminary evaluation, in this context.",2023,,[2310.13976] Advancing Requirements Engineering through Generative AI: Assessing the Role of LLMs (arxiv.org),Haowei,LLMinRE Survey,,LLMinRE,NO,,,,,,,,,,,,,,,,,,,,,,,,,,
51,Axel van Lamsweerde,An iterative approach for model-based requirements engineering in large collaborative projects: A detailed experience report,"In this paper, we report on our 7 years of practical experience designing, developing, deploying, using, and evolving an iterative Model-based Requirements Engineering (MBRE) approach and language in the context of five large European collaborative projects providing complex software-intensive solutions. Based on significant data sets collected both during project execution and via surveys realized afterward, we demonstrate that such a model-based approach can bring interesting benefits in terms of scalability (e.g., a large number of handled requirements), heterogeneity (e.g., partners with different types of RE background), adaptability and extensibility (e.g., to various project's needs), traceability (e.g., from the requirements to the software components), automation (e.g., documentation generation), consistency and quality (e.g., central model), and usefulness or usability (e.g., actual deployment and practical use). Along the way, we illustrate the application of our MBRE approach and language with concrete elements from these several European collaborative projects. More broadly, we discuss the general benefits and current limitations of using such a model-based approach and corresponding language, as well as the related lessons we learned during these past years.",2024,Science of Computer Programming,https://www.sciencedirect.com/science/article/pii/S0167642323001296?casa_token=EieCxyxyd-YAAAAA:zs13oADgBm0ZTTSHkBI0SOJVod2EAzjkB47P0B240Es_TrW9-n9JwFnMpnNXOmzLdw4lfzE_htPu,Haowei,RE Report,,,,,,,,,,,,,,,,,,,,,,,,,,,,,